# -*- coding: utf-8 -*-
"""GPT-J-6B Inference Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb

# GPT-J-6B Inference Demo

<a href="http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

This notebook demonstrates how to run the [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/#GPT-J-6B). See the link for more details about the model, including evaluation metrics and credits.

## Install Dependencies

First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).

!!! **Make sure you are using a TPU runtime!** !!!
"""

#!apt install zstd

# the "slim" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory
#!time wget https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd

#!time tar -I zstd -xf step_383500_slim.tar.zstd

#!git clone https://github.com/kingoflolz/mesh-transformer-jax.git
#!pip install -r mesh-transformer-jax/requirements.txt

# jax 0.2.12 is required due to a regression with xmap in 0.2.13
#!pip install mesh-transformer-jax/ jax==0.2.12

"""## Setup Model

"""

# import os
# import requests 
# from jax.config import config

# colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]
# url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'
# requests.post(url)

# # The following is required to use TPU Driver as JAX's backend.
# config.FLAGS.jax_xla_backend = "tpu_driver"
# config.FLAGS.jax_backend_target = "grpc://" + os.environ['COLAB_TPU_ADDR']

"""Sometimes the next step errors for some reason, just run it again ¯\\\_(ツ)\_/¯"""

import time

import jax
from jax.experimental import maps
import numpy as np
import optax
import transformers

from mesh_transformer.checkpoint import read_ckpt
from mesh_transformer.sampling import nucleaus_sample
from mesh_transformer.transformer_shard import CausalTransformer

params = {
  "layers": 28,
  "d_model": 4096,
  "n_heads": 16,
  "n_vocab": 50400,
  "norm": "layernorm",
  "pe": "rotary",
  "pe_rotary_dims": 64,

  "seq": 2048,
  "cores_per_replica": 1,
  "per_replica_batch": 1,
}

per_replica_batch = params["per_replica_batch"]
cores_per_replica = params["cores_per_replica"]
seq = params["seq"]


params["sampler"] = nucleaus_sample

# here we "remove" the optimizer parameters from the model (as we don't need them for inference)
params["optimizer"] = optax.scale(0)
print(str(jax.device_count()))
print(str(jax.devices()))
#mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)
mesh_shape = (1,1)
devices = np.array(jax.devices()).reshape(mesh_shape)

maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))

tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')

"""Here we create the network and load the parameters from the downloaded files. Expect this to take around 5 minutes."""

total_batch = per_replica_batch * jax.device_count() // cores_per_replica
print("total_batch: "+str(total_batch))
network = CausalTransformer(params)

#network.state = read_ckpt(network.state, "/mnt/c/Users/Lemming/Downloads/models/step_383500/", devices.shape[1])
#network.state = read_ckpt(network.state, "/mnt/c/Users/Lemming/Downloads/models/step_383500_slim/", 8)
network.state = read_ckpt(network.state, "step_383500/", 8, shards_out=cores_per_replica)

#network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))

"""## Run Model

Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.

Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed).

You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.

*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*
"""

def infer(context, top_p=0.9, temp=1.0, gen_len=512):
    tokens = tokenizer.encode(context)

    provided_ctx = len(tokens)
    pad_amount = seq - provided_ctx

    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)
    batched_tokens = np.array([padded_tokens] * per_replica_batch)
    length = np.ones(per_replica_batch, dtype=np.uint32) * len(tokens)

    start = time.time()
    output = network.generate(batched_tokens, length, gen_len, {"top_p": np.ones(per_replica_batch) * top_p, "temp": np.ones(per_replica_batch) * temp})

    samples = []
    decoded_tokens = output[1][0]

    for o in decoded_tokens[:, :, 0]:
      samples.append(tokenizer.decode(o))

    print(f"completion done in {time.time() - start:06}s")
    return samples

print(infer("EleutherAI is")[0])

#@title  { form-width: "300px" }
top_p = 0.9 #@param {type:"slider", min:0, max:1, step:0.1}
temp = 0.7 #@param {type:"slider", min:0, max:1, step:0.1}

print("top_p = "+str(top_p)+" min:0, max:1, step:0.1")
print("temp = "+str(temp)+" min:0, max:1, step:0.1")

while(1):
  prompt = input("> ")
  print(infer(top_p=top_p, temp=temp, gen_len=512, context=prompt)[0])

#context = """In a shocking finding,the unicorns spoke perfect English."""
#print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0])